---
title: "Google Bigquery Tutorials"
output:
  html_document:
    toc: true
    theme: united
author: Nana Boateng
df_print: paged
Time: '`r Sys.time()`'
date: "`r format(Sys.time(), '%B %d, %Y')`"
toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
number_sections: true  ## if you want number sections at each table header
highlight: tango  # specifies the syntax highlighting style
---



```{r setup,include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      out.width ="100%",
                      message = FALSE,
                      fig.align = 'default', 
                      warning = FALSE, 
                      fig.cap ="Fig. 30", 
                      out.width="100%")

options(repr.plot.height = 5, repr.plot.width = 6)
options(tidyverse.quiet = TRUE)

```


### bigrquery
The bigrquery packages provides an R interface to Google BigQuery. It makes it easy to retrieve metadata about your projects, datasets, tables and jobs, and provides a convenient wrapper for working with bigquery from R.

### Authentication
The first time you use bigrquery in a session, it will ask you to authorize bigrquery in the browser. This gives bigrquery the credentials to access data on your behalf. By default, bigrquery picks up httr's policy of caching per-working-directory credentials in .httr-oauth.

Note that bigrquery requests permission to modify your data; in general, the only data created or modified by bigrquery are the temporary tables created as query results, unless you explicitly modify your own data (say by calling delete_table() or insert_upload_job()).

```{r}
pacman::p_load(bigrquery)


#AUTH:AIzaSyCU9XhK6VmHcmrUjaU8xwRm-0rWleEuwiE
#client id::195397749877-rs4mud4m19m8qe0j525828nj6r1h1e0i.apps.googleusercontent.com
#put your project ID here::bigqueryproject1-189300
#secret::OPnLFhJzuq9X_8JJirF3vINd

#API: AIzaSyCvEUTjguSx-djmsV9Vw9I5_F91-ARE-u4
```


This opens a web browser which ask you to authenticate your account.

sample tables (samples)[https://cloud.google.com/bigquery/sample-tables]


```{r}
library(bigrquery)
project <- "bigqueryproject1-189300" # put your project ID here
sql <- "SELECT year, month, day, weight_pounds FROM [publicdata:samples.natality] LIMIT 5"
query_exec(sql, project = project)



```


```{r}
#devtools::install_github("rstats-db/bigrquery")

# install.packages('devtools') devtools::install_github("rstats-db/bigrquery")

# Use your project ID here
project <- "your-project-id" # put your project ID here

# Example query - select copies of files with content containing "TODO"
sql <- "SELECT SUM(copies)
FROM `bigquery-public-data.github_repos.sample_contents`
WHERE NOT binary AND content LIKE '%TODO%'"

# Execute the query and store the result
todo_copies <- query_exec(sql, project = project, useLegacySql = FALSE)

```



```{r}
project <- "worldbank-194204" # put your project ID here
sql <- "
SELECT
  country_name,
  ROUND(AVG(value),2) AS average
FROM
  [publicdata.worldbank.intldebt.international.debt]
WHERE
  indicator_code = SP.DYN.SMAM.FE
  AND year > 2000
GROUP BY
  country_name
ORDER BY
  average"
query_exec(sql, project = project)

#bigquery-public-data.samples.natality
# [publicdata.samples.natality]
```


```{r}
library(bigrquery)
#project <- "bigqueryproject1-189300" # put your project ID here
#sql <- "SELECT year, month, day, weight_pounds FROM [publicdata:samples.natality] LIMIT 5"
#query_exec(sql, project = project)

project <- "bigqueryproject1-189300" # put your project ID here
sql <- "SELECT
  weight_pounds, state, year, gestation_weeks
FROM
  [publicdata.samples.natality]
ORDER BY weight_pounds DESC
LIMIT 10"

query_exec(sql, project = project)


```




```{r}
project <- "bigqueryproject1-189300" # put your project ID here
sql <- "SELECT title,contributor_username,comment FROM [publicdata:samples.wikipedia] LIMIT 5"
query_exec(sql, project = project)


```



```{r}


"SELECT *
FROM [bigquery-public-data:github_repos.files]
WHERE lower(RIGHT(path, 2)) = '.r'"



```



```{r}
src <- list(project_id = "publicdata", dataset_id = "samples", table_id = "shakespeare") 

dest <- list(project_id = "bigqueryproject1-189300", dataset_id = "mydata", table_id = "shakespeare")

doubled <- dest
doubled$table_id <- "double_shakespeare"

#copy_table(src, dest)
copy_table(list(src, dest), doubled)
```


```{r}
#install.packages("assertthat")
#devtools::install_github("bigrquery")
```


```{r}
install.packages('devtools')
install.packages('httpuv')
 
devtools::install_github("assertthat")
devtools::install_github("bigrquery")
 
library(bigrquery)
require(gridExtra)
require(ggplot2)
 
 
project &lt;- "INSERT PROJECT ID HERE"
dataset &lt;- "INSERT DATASET ID HERE"
 
 
 
 
#####################################
# Big Query queries
# note
# you also need to set your dataset id
# for TABLE_DATE_RANGE in each query
# below. replace "dataset" with the id
#####################################
 
sql1&lt;- "SELECT date, totals.pageviews AS total_pageviews_per_user,
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_], TIMESTAMP('2014-06-01'),
	TIMESTAMP('2014-06-14')))
WHERE totals.transactions &gt;=1 AND totals.pageviews &gt;=0
ORDER BY fullVisitorId LIMIT 1000"
 
data1 &lt;- query_exec(project,dataset,sql1, billing = project)
 
 
 
sql2&lt;- "SELECT date, totals.pageviews AS total_pageviews_per_user,
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_], TIMESTAMP('2014-06-01'),
	TIMESTAMP('2014-06-14')))
WHERE totals.transactions IS NULL AND totals.pageviews &gt;=0
ORDER BY fullVisitorId LIMIT 1000"
 
data2 &lt;- query_exec(project,dataset,sql2, billing = project)
 
 
 
sql3 &lt;- "SELECT trafficSource.medium AS medium, count(*) AS sessions,
sum(totals.transactionRevenue)/1000000 AS total_rev,
sum(totals.transactionRevenue)/(count(*)*1000000)
AS avg_rev_per_visit
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_], TIMESTAMP('2014-06-01'),
TIMESTAMP('2014-06-07')))
GROUP BY medium
ORDER BY avg_rev_per_visit DESC LIMIT 10;"
 
data3 &lt;- query_exec(project,dataset,sql3, billing = project)
 
 
 
sql4 &lt;- "SELECT trafficSource.campaign AS campaign, count(*) AS sessions,
sum(totals.transactionRevenue)/1000000 AS total_rev,
sum(totals.transactionRevenue)/(count(*)*1000000)
AS avg_rev_per_visit
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_], TIMESTAMP('2014-06-01'),
TIMESTAMP('2014-06-07')))
GROUP BY campaign HAVING sessions &gt;= 10
ORDER BY avg_rev_per_visit DESC LIMIT 10;"
 
data4 &lt;- query_exec(project,dataset,sql4, billing = project)
 
 
 
sql5 &lt;- "SELECT trafficSource.medium AS medium,
hits.item.productCategory AS category, count(*) as value
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_],
	TIMESTAMP('2014-06-01'),
TIMESTAMP('2014-06-02'))) WHERE hits.item.productCategory IS NOT NULL
GROUP BY medium, category
ORDER BY value DESC;"
 
data5 &lt;- query_exec(project,dataset,sql5, billing = project)
 
 
 
sql7 &lt;- "
SELECT prod_name, count(*) as transactions
FROM
(
SELECT fullVisitorId, min(date) AS date, visitId,
hits.item.productName as prod_name
FROM (
SELECT fullVisitorId, date, visitId, totals.transactions,
hits.item.productName FROM
(TABLE_DATE_RANGE([dataset.ga_sessions_], TIMESTAMP('2014-06-01'),
TIMESTAMP('2014-06-14')))
)
WHERE fullVisitorId IN
(
SELECT fullVisitorId
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_], TIMESTAMP('2014-06-01'),
TIMESTAMP('2014-06-14')))
GROUP BY fullVisitorId
HAVING SUM(totals.transactions) &gt; 1
)
AND hits.item.productName IS NOT NULL
GROUP BY fullVisitorId, visitId, prod_name ORDER BY fullVisitorId DESC
)
GROUP BY prod_name ORDER BY transactions DESC;"
data7 &lt;- query_exec(project,dataset,sql7, billing = project)
 
 
 
sql8 &lt;- " SELECT hits.item.productName AS prod_name,
count(*) AS transactions
FROM (TABLE_DATE_RANGE([dataset.ga_sessions_],
	TIMESTAMP('2014-06-01'),
                       TIMESTAMP('2014-06-14')))
WHERE hits.item.productName IS NOT NULL
GROUP BY prod_name ORDER BY transactions DESC;"
 
data8 &lt;- query_exec(project,dataset,sql8, billing = project)
 
 
 
#################################
#processing
#################################
 
data9 &lt;- merge(data7,data8,by.x="prod_name",by.y="prod_name")
data9$perc &lt;- data9$transactions.x/data9$transactions.y
data9 &lt;- data9[with(data9, order(-transactions.y, perc)), ]
data9 &lt;- data9[1:10,]
 
 
 
 
##################################
# BEGIN PLOTTING
##################################
 
pdf("my first bigquery-r report.pdf",width=8.5,height=11)
 
y_max &lt;- max(max(data1$total_pageviews_per_user),
max(data2$total_pageviews_per_user))/2
 
#plot 1
p1&lt;- ggplot(data1, aes(paste(substr(date,1,4),"-",substr(date,5,6),"-",
	substr(date,7,8),sep=""),total_pageviews_per_user)) + geom_boxplot()
p1 &lt;- p1 + labs(title="Avg Pageviews for Users with Purchase",
	x = "Date", y = "Pageviews")
p1 &lt;- p1 + ylim(c(0,y_max))
p1 &lt;- p1 + theme(axis.text.x = element_text(angle = 35, hjust = 1))
 
#plot 2
p2&lt;- ggplot(data2, aes(paste(substr(date,1,4),"-",substr(date,5,6),"-",
	substr(date,7,8),sep=""),total_pageviews_per_user)) + geom_boxplot()
p2 &lt;- p2 + labs(title="Avg Pageviews for Users without Purchase",
	x = "Date", y = "Pageviews")
p2 &lt;- p2 + ylim(c(0,y_max))
p2 &lt;- p2 + theme(axis.text.x = element_text(angle = 35, hjust = 1))
 
grid.arrange(p1,p2,nrow=2)
 
 
 
 
#plot 3
par(mfrow=c(1,1))
p3&lt;- ggplot(data3, aes(x = medium,y=avg_rev_per_visit,
	fill=sessions,label=sessions))+
geom_bar(stat="identity")+scale_fill_gradient(low = colors()[600],
	high = colors()[639])
p3 &lt;- p3 + geom_text(data=data3,aes(x=medium,
	label=paste("$",round(avg_rev_per_visit,2)),
	vjust=-0.5))
p3 &lt;- p3+ labs(title="Avg Revenue/Session by Medium", x = "medium",
 y = "Avg Reveune per Session")
p3 + theme(plot.title = element_text(size = rel(2)))
 
 
 
 
#plot 4
par(mfrow=c(1,1))
p4&lt;- ggplot(data4, aes(x = campaign,y=avg_rev_per_visit,fill=sessions,
	label=sessions))+geom_bar(stat="identity")+
scale_fill_gradient(low = colors()[600], high = colors()[639])
p4 &lt;- p4 + geom_text(data=data4,aes(x=campaign,label=paste("$",
	round(avg_rev_per_visit,2)),vjust=-0.5))
p4 &lt;- p4 + labs(title="Top Campaigns by Rev/Session (min 10 sess)",
 x = "campaign", y = "Avg Reveune per Session")
p4 &lt;- p4 + theme(plot.title = element_text(size = rel(2)))
p4 + theme(axis.text.x = element_text(angle = 25, hjust = 1))
 
 
 
 
 
#plot 5
par(mfrow=c(1,1))
p5 &lt;- qplot(category,data=data5, weight=value, geom="histogram",
	fill = medium, horizontal=TRUE)+coord_flip()
p5 &lt;- p5 + labs(title="Category of Product Purchase by Medium (a)",
 x = "Product Category", y = "Transactions")
p5 &lt;- p5+ theme(plot.title = element_text(size = rel(2)))
p5 + theme(axis.text.x = element_text(angle = 25, hjust = 1))
 
 
 
 
#plot 6
par(mfrow=c(1,1))
p6 &lt;- ggplot(data5, aes(medium,category)) + geom_tile(aes(fill=value))+
scale_fill_gradient(name="transactions",low = "grey",high = "blue")
p6 &lt;- p6 + labs(title="Category of Product Purchase by Medium (b)",
	x = "medium", y = "product category")
p6 + theme(plot.title = element_text(size = rel(2)))
 
 
 
 
#plot 7
par(mfrow=c(1,1))
p7&lt;- ggplot(data9, aes(x = prod_name,y=perc,fill=transactions.y,
	label=perc))+
geom_bar(stat="identity")+scale_fill_gradient(name="transactions",
	low = colors()[600], high = colors()[639])
p7 &lt;- p7 + geom_text(data=data9,aes(x=prod_name,
	label=paste(round(perc*100,1),"%"),vjust=-0.5))
p7 &lt;- p7 + labs(title="Percentage of Transactions resulting in Future rn
	Additional Transactions by same User", x = "Product Name",
	y = "Percentage of Transactions")
#p7 &lt;- p7 + theme(plot.title = element_text(size = rel(2)))
p7 + theme(axis.text.x = element_text(angle = 45, hjust = 1))
 
 
dev.off()
```


#### Introduction
Companies using Google BigQuery for production analytics often run into the following problem: the company has a large user hit table that spans many years. Since queries are billed based on the fields accessed, and not on the date-ranges queried, queries on the table are billed for all available days and are increasingly wasteful.


A solution is to partition the table by date, so that users can query a particular range of dates; saving costs and decreasing query duration. Partitioning an un-partitioned table can be expensive if done the brute-force way. This article explores one cost-effective partitioning method, and uses the condusco R Package to automate the query generation and partitioning steps.

Migrating non-partitioned tables to partitioned tables in Google BigQuery
Let’s implement the accepted solution on StackOverflow for migrating from non-partitioned to partitioned tables in Google BigQuery.

The brute-force way to partition a non-partitioned table is to repeatedly query the table for anything matching a particular day and then save that data to a new table with the date suffix, ie. _20171201.

The problem is the cost for this method is the cost of querying the full table’s worth of data, multiplied by the number of days it needs to be partitioned into. For a 10 Terabyte table spanning three years, one SELECT * might cost $50 (BigQuery charges $5 per TB accessed). Hence, splitting the table into three years of daily partitions will cost $50*365*3 = $54,750!

The more cost-effective solution described on StackOverflow is to ARRAY_AGG the entire table into one record for each day. This requires one query over the table’s data to ARRAY_AGG each day you are interested in, and then multiple UNNEST queries using a single query on a single column.

This solution queries the full table’s worth of data twice, instead of the number of days. That’s a cost of $100, saving $54,650.

Here is an implementation of the solution using condusco to automate both the query generation and the partitioning:

```{r}


library(whisker)
library(bigrquery)
library(condusco)

# Set GBQ project
project <- ''

# Configuration
config <- data.frame(
  dataset_id = '',
  table_prefix = 'tmp_test_partition'
)

# Set the following options for GBQ authentication on a cloud instance
options("httr_oauth_cache" = "~/.httr-oauth")
options(httr_oob_default=TRUE)

# Run the below query to authenticate and write credentials to .httr-oauth file
query_exec("SELECT 'foo' as bar",project=project);

# The pipeline that creates the pivot table
migrating_to_partitioned_step_001_create_pivot <- function(params){
  
  destination_table <- "{{{dataset_id}}}.{{{table_prefix}}}_partitions"
  
  query <- "
  SELECT
    {{#date_list}}
    ARRAY_CONCAT_AGG(CASE WHEN d = 'day{{{yyyymmdd}}}' THEN r END) AS day_{{{yyyymmdd}}},
    {{/date_list}}
    line
  FROM (
    SELECT d, r, ROW_NUMBER() OVER(PARTITION BY d) AS line
    FROM (
      SELECT 
        stn, CONCAT('day', year, mo, da) AS d, ARRAY_AGG(t) AS r
      FROM `bigquery-public-data.noaa_gsod.gsod2017` AS t 
      GROUP BY stn, d
    ) 
  )
  GROUP BY line
  "
  
  query_exec(whisker.render(query,params),
             project=project,
             destination_table=whisker.render(destination_table, params),
             write_disposition='WRITE_TRUNCATE',
             use_legacy_sql = FALSE
  );
  
}

# Run the pipeline that creates the pivot table

# Create a JSON string in the invocation query that looks like [{"yyyymmdd":"20171206"},{"yyyymmdd":"20171205"},...]
invocation_query <- "
  SELECT
    '{{{dataset_id}}}' as dataset_id,
    '{{{table_prefix}}}' as table_prefix,
    CONCAT(
      '[',
      STRING_AGG(
        CONCAT('{\"yyyymmdd\":\"',FORMAT_DATE('%Y%m%d',partition_date),'\"}')
      ),
      ']'
    ) as date_list
  FROM (
    SELECT
    DATE_ADD(DATE(CURRENT_DATETIME()), INTERVAL -n DAY) as partition_date
    FROM (
      SELECT [1,2,3] as n
    ),
    UNNEST(n) AS n
  )
"

run_pipeline_gbq(
  migrating_to_partitioned_step_001_create_pivot,
  whisker.render(invocation_query,config),
  project,
  use_legacy_sql = FALSE
)

# The pipeline that creates the individual partitions 
migrating_to_partitioned_step_002_unnest <- function(params){
  
  destination_table <- "{{{dataset_id}}}.{{{table_prefix}}}_{{{day_partition_date}}}"
  
  query <- "
    SELECT r.*
    FROM {{{dataset_id}}}.{{{table_prefix}}}_partitions, UNNEST({{{day_partition_date}}}) as r
  "
  
  query_exec(whisker.render(query,params),
             project=project,
             destination_table=whisker.render(destination_table, params),
             write_disposition='WRITE_TRUNCATE',
             use_legacy_sql = FALSE
  );
  
}

invocation_query <- "
  SELECT
    '{{{dataset_id}}}' as dataset_id,
    '{{{table_prefix}}}' as table_prefix,
    CONCAT('day_',FORMAT_DATE('%Y%m%d',partition_date)) as day_partition_date
  FROM (
    SELECT
      DATE_ADD(DATE(CURRENT_DATETIME()), INTERVAL -n DAY) as partition_date
    FROM (
      SELECT [1,2,3] as n
    ),
    UNNEST(n) AS n
  )
"
run_pipeline_gbq(
  migrating_to_partitioned_step_002_unnest,
  whisker.render(invocation_query,config),
  project,
  use_legacy_sql = FALSE
)
```




```{r}
library(bigrquery)

con <- DBI::dbConnect(bigquery(), project = bq_test_project())
sql <- "SELECT 
  [1, 2, 3] as list,
  [STRUCT(1 as a, 'a' as b), STRUCT(2, 'b'), STRUCT(3, 'c')] as df
"
out <- DBI::dbGetQuery(con, sql)
#> Auto-refreshing stale OAuth token.
out
#> # A tibble: 1 x 2
#>   list      df              
#>   <list>    <list>          
#> 1 <int [3]> <tibble [3 × 2]>

out$list[[1]]
#> [1] 1 2 3

out$df[[1]]


```

